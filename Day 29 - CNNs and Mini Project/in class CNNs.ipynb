{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Neural Networks\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Building Our First Neural Network with Keras\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas matplotlib seaborn scikit-learn jupyter ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a simple binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    Dense(8, activation='relu'),                                   # Hidden layer\n",
    "    Dense(1, activation='sigmoid')                                  # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Visualizing Model Performance\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "To understand how the model is performing over time, let's plot the **training** and **validation** loss and accuracy across epochs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##  Making Predictions with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X, y)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model.save(\"tf_intro_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the same model to make predictions\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"tf_intro_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded model\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "print(f\"Loaded Model Test Loss: {test_loss}\")\n",
    "print(f\"Loaded Model Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the loaded model\n",
    "predictions = loaded_model.predict(X_test[:5])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Create your own Neural Networks\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "**1. Start with importing libraries and dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kagglehub # if kagglehub is not installed yet\n",
    "#import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "path = Path(path)\n",
    "\n",
    "list(path.iterdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "path = Path(path)\n",
    "\n",
    "csv_file = list(path.glob(\"*.csv\"))[0]\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "path = Path(path)\n",
    "\n",
    "# Auto-detect CSV file\n",
    "csv_file = list(path.glob(\"*.csv\"))[0]\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "2. **Data Cleaning**\n",
    "- We can check for missing values and handle them if needed\n",
    "- We need to turn catagorical values into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "# Drop unnecessary columns (e.g., customerID)\n",
    "data = data.drop(columns=['customerID'])\n",
    "\n",
    "# Handle missing or invalid values\n",
    "# Convert 'TotalCharges' to numeric, replacing errors with NaN\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "data = data.dropna()  # Drop rows with NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # check what kind of data there is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "- **Encode Categorical Features**\n",
    "- Categorical features need to be encoded into numerical values. You can use OneHotEncoder for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
    "                       'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "                       'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                       'StreamingMovies', 'Contract', 'PaperlessBilling', \n",
    "                       'PaymentMethod', 'Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to categorical columns\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    data[col] = encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**Select the target feature**\n",
    "- standardize the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=['Churn'])  # Features\n",
    "y = data['Churn']                 # Target (Churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numeric columns\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "**3. Train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "**4. Create a Neural Network:**\n",
    "\n",
    "A simple 3-layer neural network with two hidden layers:\n",
    "- 16 neurons in the first hidden layer.\n",
    "- 8 neurons in the second hidden layer.\n",
    "\n",
    "Output layer uses the sigmoid activation function for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(19,)),  # Input layer\n",
    "    Dense(8, activation='relu'),                                   # Hidden layer\n",
    "    Dense(1, activation='sigmoid')                                # Output layer (binary classification)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "**5. Training the model:**\n",
    "- Trained the model for 10 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "path = Path(path)\n",
    "\n",
    "csv_file = list(path.glob(\"*.csv\"))[0]\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "print(data.shape)  # mesti ~ (7043, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "data = data.dropna(subset=['TotalCharges'])\n",
    "\n",
    "print(data.shape)  # mesti > 7000 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)     # > 7000 rows\n",
    "print(X.shape)        # > 7000 rows\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Churn'].unique())\n",
    "print(data['Churn'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari column yang masih string / object\n",
    "obj_cols = X.select_dtypes(include='object').columns\n",
    "print(obj_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "# Load dataset\n",
    "path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "path = Path(path)\n",
    "csv_file = list(path.glob(\"*.csv\"))[0]\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Encode target\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Fix TotalCharges\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "data = data.dropna(subset=['TotalCharges'])\n",
    "\n",
    "# Drop ID\n",
    "data = data.drop('customerID', axis=1)\n",
    "\n",
    "# Split X y\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# One-hot encode\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(X.select_dtypes(include='object'))  # mesti kosong\n",
    "print(y.unique(), y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### **We got an error**\n",
    "Let's debug this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.isnan(X_train).any())  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(include='object')\n",
    "# Index([], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)\n",
    "# numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))     # numpy.ndarray\n",
    "print(X_train.dtype)    # float64 / float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))     # numpy.ndarray\n",
    "print(X_train.dtype)    # float32 / float64\n",
    "\n",
    "print(type(y_train))    # numpy.ndarray\n",
    "print(y_train.dtype)    # int64 / float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.isnan(X_train).any())  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'OnlineBackup' column\n",
    "# 1. Encode target\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 2. One-hot encode features\n",
    "X = pd.get_dummies(data.drop('Churn', axis=1), drop_first=True)\n",
    "\n",
    "# 3. Scale â†’ NumPy\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)  # All columns should now be numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtype)  # float32 / float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "**Proceed with training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Non-numeric columns in X (pandas):\")\n",
    "print(X.select_dtypes(exclude=['number', 'bool']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))      # numpy.ndarray\n",
    "print(X_train.dtype)     # float64 / float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X_train).any())   # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('OnlineBackup', axis=1)\n",
    "X_train = X_train.drop('InternetService', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)  # All columns should now be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "istory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  # Check the number of columns (should be 18 now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(17,)),  # Adjusted to 17 features\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)  # Check the data types of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop multiple columns\n",
    "X_test = X_test.drop(['InternetService', 'OnlineBackup'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accuracy\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (model outputs probabilities for class 1)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "# This comparison checks if the predicted probability for class 1 is greater than 0.5. If it is, the output is True (1), else False (0).\n",
    "# .astype(int): Converts the boolean True/False values into 1/0 for final binary classification.\n",
    "\n",
    "\n",
    "# If you want the predictions in a more readable format\n",
    "print(predictions[:10])  # Print first 10 predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
